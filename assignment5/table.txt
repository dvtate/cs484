Layers	Neurons	ActFn	MCR	Loss	Iter
1	1	iden	0.25	0.66629	8
1	1	logi	0.5	0.585204	68
1	1	relu	0.14	0.584713	27
1	1	tanh	0.28	0.693148	5
1	2	iden	0.25	0.66629	9
1	2	logi	0.39	0.474326	288
1	2	relu	0.5	0.520739	41
1	2	tanh	0.19	0.544104	35
1	3	iden	0.25	0.666291	6
1	3	logi	0.4	0.351642	142
1	3	relu	0.38	0.42305	101
1	3	tanh	0.44	0.412109	45
1	4	iden	0.25	0.666291	5
1	4	logi	0.4	0.257223	360
1	4	relu	0.31	0.359374	55
1	4	tanh	0.5	0.039411	1696
1	5	iden	0.25	0.666291	5
1	5	logi	0.5	0.008901	1384
1	5	relu	0.42	0.288185	45
1	5	tanh	0.5	0.004966	4117
1	6	iden	0.25	0.666291	7
1	6	logi	0.5	0.002225	257
1	6	relu	0.5	0.000552	97
1	6	tanh	0.5	0.000924	91
1	7	iden	0.25	0.666291	6
1	7	logi	0.5	0.002388	227
/usr/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=2):					
ABNORMAL_TERMINATION_IN_LNSRCH.					
					
Increase the number of iterations (max_iter) or scale the data as shown in:					
https://scikit-learn.org/stable/modules/preprocessing.html					
self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)					
1	7	relu	0.5	0.029238	91
1	7	tanh	0.5	0.001518	195
1	8	iden	0.25	0.666292	6
1	8	logi	0.5	0.002066	335
1	8	relu	0.5	0.000662	102
1	8	tanh	0.5	0.001917	546
1	9	iden	0.25	0.666292	6
1	9	logi	0.5	0.001796	159
1	9	relu	0.5	0.00052	69
1	9	tanh	0.5	0.001039	124
1	10	iden	0.25	0.666292	6
1	10	logi	0.5	0.00316	356
1	10	relu	0.5	0.000356	66
1	10	tanh	0.5	0.001172	235
2	1	iden	0.25	0.666291	10
2	1	logi	0.5	0.585196	99
2	1	relu	0.5	0.584768	26
2	1	tanh	0.5	0.584792	28
2	2	iden	0.25	0.666291	10
2	2	logi	0.43	0.578534	116
2	2	relu	0.5	0.500207	27
2	2	tanh	0.31	0.54982	227
2	3	iden	0.25	0.666291	8
2	3	logi	0.4	0.305488	1546
2	3	relu	0.38	0.429885	64
2	3	tanh	0.5	0.24032	1993
2	4	iden	0.25	0.666292	9
2	4	logi	0.5	0.223117	351
2	4	relu	0.5	0.000177	152
2	4	tanh	0.5	0.222328	240
2	5	iden	0.25	0.666294	9
2	5	logi	0.5	0.001207	380
2	5	relu	0.48	0.099705	91
2	5	tanh	0.5	0.000218	161
2	6	iden	0.25	0.666295	6
2	6	logi	0.5	0.000895	245
2	6	relu	0.5	0.040163	61
2	6	tanh	0.5	0.001353	861
2	7	iden	0.25	0.666295	8
2	7	logi	0.5	0.001111	133
2	7	relu	0.5	0.000183	49
2	7	tanh	0.5	0.002747	799
2	8	iden	0.25	0.666297	6
2	8	logi	0.5	0.00064	301
2	8	relu	0.49	0.022905	107
2	8	tanh	0.5	0.000463	72
2	9	iden	0.25	0.666298	6
2	9	logi	0.5	0.001344	398
2	9	relu	0.5	0.000213	49
2	9	tanh	0.5	0.000351	111
2	10	iden	0.25	0.666297	6
2	10	logi	0.5	0.001627	569
2	10	relu	0.5	0.000364	63
2	10	tanh	0.5	0.000538	67
3	1	iden	0.25	0.66629	14
3	1	logi	0.14	0.585228	57
3	1	relu	0	0.693149	4
3	1	tanh	0.15	0.575982	34
3	2	iden	0.25	0.666293	10
3	2	logi	0.24	0.557091	236
3	2	relu	0	0.693151	3
3	2	tanh	0.23	0.499018	37
3	3	iden	0.25	0.666293	8
3	3	logi	0.35	0.242617	1782
3	3	relu	0.4	0.418596	113
3	3	tanh	0.5	0.284399	295
3	4	iden	0.25	0.666295	6
3	4	logi	0.5	0.006326	5723
3	4	relu	0.44	0.301256	172
3	4	tanh	0.45	0.102142	559
3	5	iden	0.25	0.666297	6
3	5	logi	0.5	0.000967	328
3	5	relu	0.5	0.000752	121
3	5	tanh	0.5	0.000345	811
3	6	iden	0.25	0.666299	6
3	6	logi	0.5	0.000636	477
3	6	relu	0.5	0.076595	65
3	6	tanh	0.5	0.000204	71
3	7	iden	0.25	0.666299	6
3	7	logi	0.5	0.002871	1854
3	7	relu	0.5	0.000162	43
3	7	tanh	0.5	0.000298	73
3	8	iden	0.25	0.666301	7
3	8	logi	0.5	0.000871	172
3	8	relu	0.5	0.000315	55
3	8	tanh	0.5	0.0002	60
3	9	iden	0.25	0.666301	8
3	9	logi	0.5	0.000854	201
3	9	relu	0.5	0.000227	50
3	9	tanh	0.5	0.000169	64
3	10	iden	0.25	0.666302	7
3	10	logi	0.5	0.001377	348
3	10	relu	0.5	0.000122	39
3	10	tanh	0.5	0.00023	88
4	1	iden	0.25	0.666297	19
4	1	logi	0.22	0.693154	4
4	1	relu	0.5	0.69315	4
4	1	tanh	0.22	0.693145	6
4	2	iden	0.25	0.666293	14
4	2	logi	0.21	0.693148	2
4	2	relu	0	0.693152	3
4	2	tanh	0.33	0.358162	1798
4	3	iden	0.25	0.666296	11
4	3	logi	0.5	0.384943	2861
4	3	relu	0.5	0.278493	118
4	3	tanh	0.5	0.338484	2118
4	4	iden	0.25	0.666298	8
4	4	logi	0.5	0.693153	2
4	4	relu	0.5	0.366709	130
4	4	tanh	0.5	0.340067	325
4	5	iden	0.25	0.666301	7
4	5	logi	0.25	0.693188	2
4	5	relu	0.5	0.204778	78
4	5	tanh	0.26	0.339859	105
4	6	iden	0.25	0.666302	6
4	6	logi	0.21	0.693131	3
4	6	relu	0.5	0.000232	105
4	6	tanh	0.5	0.000201	127
4	7	iden	0.25	0.666303	7
4	7	logi	0.5	0.000664	420
4	7	relu	0.5	0.000083	53
4	7	tanh	0.5	0.000192	77
4	8	iden	0.25	0.666306	7
4	8	logi	0.31	0.693178	2
4	8	relu	0.5	0.000163	98
4	8	tanh	0.5	0.000172	49
4	9	iden	0.25	0.666308	7
4	9	logi	0.33	0.693167	3
4	9	relu	0.5	0.000168	75
4	9	tanh	0.5	0.000471	151
4	10	iden	0.25	0.666308	7
4	10	logi	0.23	0.69315	2
4	10	relu	0.5	0.000107	45
4	10	tanh	0.5	0.000286	62
5	1	iden	0.25	0.6663	33
5	1	logi	0	0.693149	3
5	1	relu	0.5	0.69315	6
5	1	tanh	0.18	0.693152	5
5	2	iden	0.25	0.666296	16
5	2	logi	0.5	0.693149	2
5	2	relu	0	0.693153	3
5	2	tanh	0.5	0.53231	416
5	3	iden	0.25	0.666296	9
5	3	logi	0.24	0.693164	2
5	3	relu	0.38	0.427076	107
5	3	tanh	0.5	0.000148	626
5	4	iden	0.25	0.666299	8
5	4	logi	0.32	0.693155	2
5	4	relu	0.42	0.526209	177
5	4	tanh	0.49	0.416915	1108
5	5	iden	0.25	0.666303	6
5	5	logi	0.06	0.693154	2
5	5	relu	0.5	0.35133	105
5	5	tanh	0.28	0.365418	380
5	6	iden	0.25	0.666305	7
5	6	logi	0.5	0.693144	2
5	6	relu	0.5	0.000227	110
5	6	tanh	0.5	0.000114	69
5	7	iden	0.25	0.666307	8
5	7	logi	0.2	0.69315	3
5	7	relu	0.5	0.026564	118
5	7	tanh	0.5	0.000122	84
5	8	iden	0.25	0.66631	7
5	8	logi	0.5	0.693159	2
5	8	relu	0.5	0.000207	84
5	8	tanh	0.5	0.000244	138
5	9	iden	0.25	0.666312	7
5	9	logi	0.35	0.693158	2
5	9	relu	0.5	0.000083	73
5	9	tanh	0.5	0.000123	60
5	10	iden	0.25	0.666313	8
5	10	logi	0.5	0.69316	2
5	10	relu	0.5	0.000149	94
5	10	tanh	0.5	0.000135	62
